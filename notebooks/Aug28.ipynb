{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# August 28\n",
    "\n",
    "Working with one dataset is all well and good, but we often want to associate information in two datasets. For example, we could have different subsets of data, with the same columns, collected at different times. Perhaps we have \n",
    "technicians going into the field at different points in time. To the `data` folder, I have also added a second file, `second_subset.csv`. These are data that a field tech forgot to add to the dataset. So we will now add them to the data set.\n",
    "\n",
    "Read in the `surveys.csv` file and the `second_subset.csv` data file. Call the variables to which you save each one `surveys_df` and `second_df`, respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief note on files\n",
    "\n",
    "So far, I've just asked you to take it on faith that our data are in `../data/surveys.csv`. But what does that mean? Let's go to the board for a minute. \n",
    "\n",
    "Exercise for once I've explained on the board:\n",
    "\n",
    "- What is the parent directory of secret_data? \n",
    "- What would be the path to secrete_data from notebooks?\n",
    "- What are the child directories of maze?\n",
    "- Can you read in the data from hidden_data?\n",
    "\n",
    "In the coming weeks, we will talk about organizing a computational research project, such that we have our scripts all in one place, our data all in one place, and our outputs all in one place. But for now, I'm just introducing a little information on where the \"../\" comes from. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll look at concatenating dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_stack = pd.concat([surveys_df, second_df], axis=0)\n",
    "vertical_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the output. What does \"concatenate\" mean? Is this the result we wanted? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_stack = vertical_stack.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reset_index()` allows us to renumber the indices, such that we now have a contiguous dataset, as opposed to two numbering systems squashed together.\n",
    "\n",
    "In the below cell, write the new dataset to a file. Where should you save it? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Data\n",
    "\n",
    "Another task we might like to do is combine two different datasets based on columns. For example, records for one specimen might be in two different files which keep track of different variables.\n",
    "\n",
    "For example, in our classroom dataset, whoever recorded to rodent data used shorthand for the species names - DO, DL, etc. That makes taking data while wrangling live animals easier. In a second file, \"species.csv\", we have a translation table to go from the shortand to the full name for analysis as we work on the paper. The goal here is to make it easier to take data in the field (by using shortand) without losing data (being able to translate back to the long form). \n",
    "\n",
    "\n",
    "When we concatenated our DataFrames we simply added them to each other - stacking them either vertically. Another way to combine DataFrames is to use columns in each dataset that contain common values (a common unique id). Combining DataFrames using a common field is called “joining”. The columns containing the common values are called “join key(s)”. Joining DataFrames in this way is often useful when one DataFrame is a “lookup table” containing additional data that we want to include in the other.\n",
    "\n",
    "Storing data in this way has many benefits including:\n",
    "\n",
    "- It ensures consistency in the spelling of species attributes (genus, species and taxa) given each species is only entered once. Imagine the possibilities for spelling errors when entering the genus and species thousands of times!\n",
    "- It also makes it easy for us to make changes to the species information once without having to find each instance of it in the larger survey data.\n",
    "- It optimizes the size of our data.\n",
    "\n",
    "## Identifying join keys\n",
    "\n",
    "To identify appropriate join keys we first need to know which field(s) are shared between the files (DataFrames). We might inspect both DataFrames to identify these columns. If we are lucky, both DataFrames will have columns with the same name that also contain the same data. If we are less lucky, we need to identify a (differently-named) column in each DataFrame that contains the same information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = pd.read_csv(\"../data/species.csv\")\n",
    "species.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, the join key is the column containing the two-letter species identifier, which is called species_id. \n",
    "\n",
    "Now that we know the fields with the common species ID attributes in each DataFrame, we are almost ready to join our data. However, since there are different types of joins, we also need to decide which type of join makes sense for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner Joins\n",
    "\n",
    "The most common type of join is called an inner join. An inner join combines two DataFrames based on a join key and returns a new DataFrame that contains only those rows that have matching values in both of the original DataFrames.\n",
    "\n",
    "Inner joins yield a DataFrame that contains only rows where the value being joins exists in BOTH tables:\n",
    "\n",
    "![inner](img/inner-join.png)\n",
    "\n",
    "The pandas function for performing joins is called merge and an Inner join is the default option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_inner = pd.merge(left=surveys_df,right=species, left_on='species_id', right_on='species_id')\n",
    "# In this case `species_id` is the only column name in  both dataframes, so if we skippd `left_on`\n",
    "# And `right_on` arguments we would still get the same result\n",
    "\n",
    "# What's the size of the output data?\n",
    "merged_inner.shape\n",
    "merged_inner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of an inner join of surveys and species is a new DataFrame that contains the combined set of columns from surveys and species. It only contains rows that have two-letter species codes that are the same in both the surveys and species DataFrames. In other words, if a row in surveys has a value of species_id that does not appear in the species_id column of species, it will not be included in the DataFrame returned by an inner join. Similarly, if a row in species has a value of species_id that does not appear in the species_id column of surveys, that row will not be included in the DataFrame returned by an inner join.\n",
    "\n",
    "The two DataFrames that we want to join are passed to the merge function using the left and right argument. The left_on='species' argument tells merge to use the species_id column as the join key from surveys (the left DataFrame). Similarly , the right_on='species_id' argument tells merge to use the species_id column as the join key from species (the right DataFrame). For inner joins, the order of the left and right arguments does not matter.\n",
    "\n",
    "The result merged_inner DataFrame contains all of the columns from surveys (record id, month, day, etc.) as well as all the columns from species (species_id, genus, species, and taxa).\n",
    "\n",
    "Notice that merged_inner has fewer rows than surveys. This is an indication that there were rows in surveys_df with value(s) for species_id that do not exist as value(s) for species_id in species_df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left Joins\n",
    "\n",
    "What if we want to add information from species to surveys_df without losing any of the information from surveys_df? In this case, we use a different type of join called a “left outer join”, or a “left join”.\n",
    "\n",
    "Like an inner join, a left join uses join keys to combine two DataFrames. Unlike an inner join, a left join will return all of the rows from the left DataFrame, even those rows whose join key(s) do not have values in the right DataFrame. Rows in the left DataFrame that are missing values for the join key(s) in the right DataFrame will simply have null (i.e., NaN or None) values for those columns in the resulting joined DataFrame.\n",
    "\n",
    "Note: a left join will still discard rows from the right DataFrame that do not have values for the join key(s) in the left DataFrame.\n",
    "\n",
    "![left join](img/left-join.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_left = pd.merge(left=surveys_df,right=species, how='left', left_on='species_id', right_on='species_id')\n",
    "merged_left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result DataFrame from a left join (merged_left) looks very much like the result DataFrame from an inner join (merged_inner) in terms of the columns it contains. However, unlike merged_inner, merged_left contains the same number of rows as the original surveys_df DataFrame. When we inspect merged_left, we find there are rows where the information that should have come from species_sub (i.e., species_id, genus, and taxa) is missing (they contain NaN values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_left[ pd.isnull(merged_left.genus) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rows are the ones where the value of species_id from survey_sub (in this case, PF) does not occur in species_sub. What might this mean about the `PF` value? \n",
    "The pandas merge function supports two other join types:\n",
    "\n",
    "- Right (outer) join: Invoked by passing how='right' as an argument. Similar to a left join, except all rows from the right DataFrame are kept, while rows from the left DataFrame without matching join key(s) values are discarded.\n",
    "\n",
    "- Full (outer) join: Invoked by passing how='outer' as an argument. This join type returns the all pairwise combinations of rows from both DataFrames; i.e., the result DataFrame will NaN where data is missing in one of the dataframes. This join type is very rarely used.\n",
    "\n",
    "Try one or both of these to get a fuller sense of what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reminder: For Loops\n",
    "\n",
    "April: Don't forget to ask about what databases people think they will use!!!!!\n",
    "\n",
    "Loops allow us to repeat a workflow (or series of actions) a given number of times or while some condition is true. We would use a loop to automatically process data that’s stored in multiple files (daily values with one file per year, for example). Loops lighten our work load by performing repeated tasks without our direct involvement and make it less likely that we’ll introduce errors by making mistakes while processing each file by hand.\n",
    "\n",
    "Let’s write a simple for loop that simulates what a kid might see during a visit to the zoo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = ['lion', 'tiger', 'crocodile', 'vulture', 'hippo']\n",
    "print(animals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for creature in animals:\n",
    "    print(creature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line defining the loop must start with for and end with a colon, and the body of the loop must be indented.\n",
    "\n",
    "In this example, creature is the loop variable that takes the value of the next entry in animals every time the loop goes around. We can call the loop variable anything we like. After the loop finishes, the loop variable will still exist and will have the value of the last entry in the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for creature in animals:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The loop variable is now: ' + creature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened above? Tell your neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick challenge:\n",
    "- What happens if we don’t include the pass statement?\n",
    "- Rewrite the loop so that the animals are separated by commas, not new lines (Hint: You can concatenate strings using a plus sign. For example, print(string1 + string2) outputs ‘string1string2’).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using loops to automate analyses\n",
    "\n",
    "The file we’ve been using so far, `surveys.csv`, contains 25 years of data and is very large. We would like to separate the data for each year into a separate file.\n",
    "\n",
    "Let’s start by making a new directory inside the folder data to store all of these files using the module os. If we want to put the directory in the data directory, what will be the path? Enter it between the quotation marks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that we've successfully created the directory with the `os.listdir` command. Fill in the path between the quotation marks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous lessons, we saw how to use the library pandas to load the species data into memory as a DataFrame, how to select a subset of the data using some criteria, and how to write the DataFrame into a CSV file. Let’s write a script that performs those three steps in sequence for the year 2002. Remember to fill in the path to your yearly_files in teh last command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "surveys_df = pd.read_csv('../data/surveys.csv')\n",
    "\n",
    "# Select only data for the year 2002\n",
    "surveys2002 = surveys_df[surveys_df.year == 2002]\n",
    "\n",
    "# Write the new DataFrame to a CSV file\n",
    "surveys2002.to_csv('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create yearly data files, we could repeat the last two commands over and over, once for each year of data. Repeating code is neither elegant nor practical, and is very likely to introduce errors into your code. We want to turn what we’ve just written into a loop that repeats the last two commands for every year in the dataset.\n",
    "\n",
    "Let’s start by writing a loop that simply prints the names of the files we want to create - the dataset we are using covers 1977 through 2002, and we’ll create a separate file for each of those years. Listing the filenames is a good way to confirm that the loop is behaving as we expect.\n",
    "\n",
    "We have seen that we can loop over a list of items, so we need a list of years to loop over. We can get the years in our DataFrame with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df['year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but we want only unique years, which we can get using the unique method which we have already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df['year'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this into a for loop we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in surveys_df['year'].unique():\n",
    "   filename='data/yearly_files/surveys' + str(year) + '.csv'\n",
    "   print(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add the rest of the steps we need to create separate text files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a DataFrame\n",
    "surveys_df = pd.read_csv('../data/surveys.csv')\n",
    "\n",
    "for year in surveys_df['year'].unique():\n",
    "\n",
    "    # Select data for the year\n",
    "    surveys_year = surveys_df[surveys_df.year == year]\n",
    "\n",
    "    # Write the new DataFrame to a CSV file\n",
    "    filename = '../data/yearly_files/surveys' + str(year) + '.csv'\n",
    "    surveys_year.to_csv(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk through the loop with a partner. Look inside the yearly_files directory and check a couple of the files you just created to confirm that everything worked as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the code above created a unique filename for each year.\n",
    "\n",
    "`filename = '../data/yearly_files/surveys' + str(year) + '.csv'`\n",
    "\n",
    "Let’s break down the parts of this name:\n",
    "\n",
    "- The first part is simply some text that specifies the directory to store our data file in (data/yearly_files/) and the first part of the file name (surveys): '../data/yearly_files/surveys'\n",
    "- We can concatenate this with the value of a variable, in this case year by using the plus + sign and the variable we want to add to the file name: + str(year)\n",
    "- Then we add the file extension as another text string: + '.csv'\n",
    "\n",
    "Notice that we use single quotes to add text strings. The variable is not surrounded by quotes. This code produces the string data/yearly_files/surveys2002.csv which contains the path to the new filename AND the file name itself.\n",
    "\n",
    "## Challenges\n",
    "\n",
    "Some of the surveys you saved are missing data (they have null values that show up as NaN - Not A Number - in the DataFrames and do not show up in the text files). Modify the for loop so that the entries with null values are not included in the yearly files.\n",
    "\n",
    "- Let’s say you only want to look at data from a given multiple of years. How would you modify your loop in order to generate a data file for only every 5th year, starting from 1977?\n",
    "\n",
    "- Instead of splitting out the data by years, a colleague wants to do analyses each species separately. How would you write a unique CSV file for each species?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a DataFrame\n",
    "surveys_df = pd.read_csv('../data/surveys.csv')\n",
    "\n",
    "for spec in surveys_df['species_id'].unique():\n",
    "\n",
    "    # Select data for the year\n",
    "    surveys_sp = surveys_df[surveys_df.species_id == spec]\n",
    "\n",
    "    # Write the new DataFrame to a CSV file\n",
    "    \n",
    "    filename = '../data/spec_files/surveys' + str(spec) + '.csv'\n",
    "    surveys_sp.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat ../data/spec_files/surveysPL.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building reusable and modular code with functions\n",
    "\n",
    "Suppose that separating large data files into individual yearly files is a task that we frequently have to perform. We could write a for loop like the one above every time we needed to do it but that would be time consuming and error prone. A more elegant solution would be to create a reusable tool that performs this task with minimum input from the user. To do this, we are going to turn the code we’ve already written into a function.\n",
    "\n",
    "Functions are reusable, self-contained pieces of code that are called with a single command. They can be designed to accept arguments as input and return values, but they don’t need to do either. Variables declared inside functions only exist while the function is running and if a variable within the function (a local variable) has the same name as a variable somewhere else in the code, the local variable hides but doesn’t overwrite the other.\n",
    "\n",
    "Every method used in Python (for example, print) is a function, and the libraries we import (say, pandas) are a collection of functions. We will only use functions that are housed within the same code that uses them, but it’s also easy to write functions that can be used by different programs.\n",
    "\n",
    "Functions are declared following this general structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def this_is_the_function_name(input_argument1, input_argument2):\n",
    "    sample_value = 4\n",
    "    sample_2 = 5\n",
    "    # The body of the function is indented\n",
    "    # This function prints the two arguments to screen\n",
    "    print('The function arguments are:', input_argument1, input_argument2, '(this is done inside the function!)')\n",
    "    # Print statements are just for looks\n",
    "    output_argument = input_argument1 * input_argument2\n",
    "    sample_variable = 5\n",
    "    # And returns their product\n",
    "    return(output_argument, sample_variable) \n",
    "\n",
    "sample_variable = 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function declaration starts with the word def, followed by the function name and any arguments in parenthesis, and ends in a colon. The body of the function is indented just like loops are. If the function returns something when it is called, it includes a return statement at the end.\n",
    "\n",
    "This is how we call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_of_inputs = this_is_the_function_name(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Their product is:', product_of_inputs, '(this is done outside the function!)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge \n",
    "\n",
    "- Change the values of the arguments in the function and check its output. What if one or both outputs are non-numeric?\n",
    "- Try calling the function by giving it the wrong number of arguments (not 2) or not assigning the function call to a variable (no `product_of_inputs =`)\n",
    "- Declare a variable inside the function and test to see where it exists (Hint: can you print it from outside the function?)\n",
    "- Explore what happens when a variable both inside and outside the function have the same name. What happens to the global variable when you change the value of the local variable?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now turn our code for saving yearly data files into a function. There are many different “chunks” of this code that we can turn into functions, and we can even create functions that call other functions inside them. Let’s first write a function that separates data for just one year and saves that data to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_year_csv_writer(this_year, all_data):\n",
    "    \"\"\"\n",
    "    Writes a csv file for data from a given year.\n",
    "\n",
    "    this_year --- year for which data is extracted\n",
    "    all_data --- DataFrame with multi-year data\n",
    "    \"\"\"\n",
    "\n",
    "    # Select data for the year\n",
    "    surveys_year = all_data[all_data.year == this_year]\n",
    "\n",
    "    # Write the new DataFrame to a csv file\n",
    "    filename = '../data/yearly_files/function_surveys' + str(this_year) + '.csv'\n",
    "    surveys_year.to_csv(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text between the two sets of triple double quotes is called a docstring and contains the documentation for the function. It does nothing when the function is running and is therefore not necessary, but it is good practice to include docstrings as a reminder of what the code does. Docstrings in functions also become part of their ‘official’ documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(one_year_csv_writer)\n",
    "#or\n",
    "#one_year_csv_writer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surveys_df = pd.read_csv(\"../data/surveys.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_year_csv_writer(2002, surveys_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We changed the root of the name of the CSV file so we can distinguish it from the one we wrote before. Check the yearly_files directory for the file. Did it do what you expect?\n",
    "\n",
    "What we really want to do, though, is create files for multiple years without having to request them one by one. Let’s write another function that replaces the entire For loop by simply looping through a sequence of years and repeatedly calling the function we just wrote, one_year_csv_writer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearly_data_csv_writer(start_year, end_year, all_data):\n",
    "    \"\"\"\n",
    "    Writes separate CSV files for each year of data.\n",
    "\n",
    "    start_year --- the first year of data we want\n",
    "    end_year --- the last year of data we want\n",
    "    all_data --- DataFrame with multi-year data\n",
    "    \"\"\"\n",
    "\n",
    "    # \"end_year\" is the last year of data we want to pull, so we loop to end_year+1\n",
    "    for year in range(start_year, end_year+1):\n",
    "        one_year_csv_writer(year, all_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because people will naturally expect that the end year for the files is the last year with data, the for loop inside the function ends at end_year + 1. By writing the entire loop into a function, we’ve made a reusable tool for whenever we need to break a large data file into yearly files. Because we can specify the first and last year for which we want files, we can even use this function to create files for a subset of the years available. This is how we call this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_data_csv_writer(1977, 2002, surveys_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions we wrote demand that we give them a value for every argument. Ideally, we would like these functions to be as flexible and independent as possible. Let’s modify the function yearly_data_csv_writer so that the start_year and end_year default to the full range of the data if they are not supplied by the user. Arguments can be given default values with an equal sign in the function declaration. Any arguments in the function without default values (here, all_data) is a required argument and MUST come before the argument with default values (which are optional in the function call)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearly_data_arg_test(all_data, start_year = 1977, end_year = 2002):\n",
    "    \"\"\"\n",
    "    Modified from yearly_data_csv_writer to test default argument values!\n",
    "\n",
    "    start_year --- the first year of data we want --- default: 1977\n",
    "    end_year --- the last year of data we want --- default: 2002\n",
    "    all_data --- DataFrame with multi-year data\n",
    "    \"\"\"\n",
    "\n",
    "    return start_year, end_year\n",
    "\n",
    "\n",
    "start,end = yearly_data_arg_test (surveys_df, 1988, 1993)\n",
    "print('Both optional arguments:\\t', start, end)\n",
    "\n",
    "start,end = yearly_data_arg_test (surveys_df)\n",
    "print('Default values:\\t\\t\\t', start, end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if our dataset doesn’t start in 1977 and end in 2002? We can modify the function so that it looks for the start and end years in the dataset if those dates are not provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearly_data_arg_test(dataset = all_data, start_year = None, end_year = None):\n",
    "    \"\"\"\n",
    "    Modified from yearly_data_csv_writer to test default argument values!\n",
    "\n",
    "    start_year --- the first year of data we want --- default: None - check all_data\n",
    "    end_year --- the last year of data we want --- default: None - check all_data\n",
    "    all_data --- DataFrame with multi-year data\n",
    "    \"\"\"\n",
    "\n",
    "    if start_year is None:\n",
    "        start_year = min(all_data.year)\n",
    "    if end_year is None:\n",
    "        end_year = max(all_data.year)\n",
    "\n",
    "    return start_year, end_year\n",
    "\n",
    "\n",
    "start,end = yearly_data_arg_test (surveys_df, 1988, 1993)\n",
    "print('Both optional arguments:\\t', start, end)\n",
    "\n",
    "start,end = yearly_data_arg_test (surveys_df)\n",
    "print('Default values:\\t\\t\\t', start, end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default values of the start_year and end_year arguments in the function yearly_data_arg_test are now None. This is a build-it constant in Python that indicates the absence of a value - essentially, that the variable exists in the namespace of the function (the directory of variable names) but that it doesn’t correspond to any existing object.\n",
    "\n",
    "# If Statements\n",
    "\n",
    "The body of the test function now has two conditionals (if statements) that check the values of start_year and end_year. If statements execute a segment of code when some condition is met. They commonly look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5\n",
    "\n",
    "if a<0:  # Meets first condition?\n",
    "\n",
    "    # if a IS less than zero\n",
    "    print('a is a negative number')\n",
    "\n",
    "elif a>0:  # Did not meet first condition. meets second condition?\n",
    "\n",
    "    # if a ISN'T less than zero and IS more than zero\n",
    "    print('a is a positive number')\n",
    "\n",
    "else:  # Met neither condition\n",
    "\n",
    "    # if a ISN'T less than zero and ISN'T more than zero\n",
    "    print('a must be zero!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the value of a to see how this function works. The statement elif means “else if”, and all of the conditional statements must end in a colon.\n",
    "\n",
    "The if statements in the function yearly_data_arg_test check whether there is an object associated with the variable names start_year and end_year. If those variables are None, the if statements return the boolean True and execute whatever is in their body. On the other hand, if the variable names are associated with some value (they got a number in the function call), the if statements return False and do not execute. The opposite conditional statements, which would return True if the variables were associated with objects (if they had received value in the function call), would be if start_year and if end_year.\n",
    "\n",
    "As we’ve written it so far, the function yearly_data_arg_test associates values in the function call with arguments in the function definition just based in their order. If the function gets only two values in the function call, the first one will be associated with all_data and the second with start_year, regardless of what we intended them to be. We can get around this problem by calling the function using keyword arguments, where each of the arguments in the function definition is associated with a keyword and the function call passes values to the function using these keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start,end = yearly_data_arg_test(surveys_df)\n",
    "print('Default values:\\t\\t\\t', start, end)\n",
    "\n",
    "start,end = yearly_data_arg_test(surveys_df, 1993, 1988)\n",
    "print('No keywords:\\t\\t\\t', start, end)\n",
    "\n",
    "start,end = yearly_data_arg_test(surveys_df, start_year = 1988, end_year = 1993)\n",
    "print('Both keywords, in order:\\t', start, end)\n",
    "\n",
    "start,end = yearly_data_arg_test(surveys_df, end_year = 1993, start_year = 1988)\n",
    "print('Both keywords, flipped:\\t\\t', start, end)\n",
    "\n",
    "start,end = yearly_data_arg_test(surveys_df, start_year = 1988)\n",
    "print('One keyword, default end:\\t', start, end)\n",
    "\n",
    "start,end = yearly_data_arg_test(surveys_df, end_year = 1993)\n",
    "print('One keyword, default start:\\t', start, end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "\n",
    "- The code below checks to see whether a directory exists and creates one if it doesn’t. Add some code to your function that writes out the CSV files, to check for a directory to write to.\n",
    "\n",
    "`if 'dir_name_here' in os.listdir('.'):\n",
    "   print('Processed directory exists')\n",
    "else:\n",
    "   os.mkdir('dir_name_here')\n",
    "   print('Processed directory created')\n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'test_output' in os.listdir('../data/'):\n",
    "   print('Processed directory exists')\n",
    "else:\n",
    "   os.mkdir('../data/test_output')\n",
    "   print('Processed directory created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_year_csv_writer(this_year, all_data):\n",
    "    \"\"\"\n",
    "    Writes a csv file for data from a given year.\n",
    "\n",
    "    this_year --- year for which data is extracted\n",
    "    all_data --- DataFrame with multi-year data\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'test_output' in os.listdir('../data/'):\n",
    "       print('Processed directory exists')\n",
    "    else:\n",
    "       os.mkdir('../data/test_output')\n",
    "       print('Processed directory created')\n",
    "        \n",
    "    # Select data for the year\n",
    "    surveys_year = all_data[all_data.year == this_year]\n",
    "\n",
    "    # Write the new DataFrame to a csv file\n",
    "    filename = '../data/test_output/function_surveys' + str(this_year) + '.csv'\n",
    "    surveys_year.to_csv(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed directory exists\n"
     ]
    }
   ],
   "source": [
    "one_year_csv_writer(2002, surveys_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Thursday\n",
    "\n",
    "Thursday, we will be doing some hands-on practice with datasets, and writing functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
